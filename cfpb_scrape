from requests import get
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep
import time
from random import randint
from IPython.core.display import clear_output
import warnings

#Create lists to store scraped data
names = []
dates = []
urls = []

#To iterate through 25 pages
pages = [str(i) for i in range(1, 26)]

#Define start time of the requests and the number of requests
start_time = time.time()
requests = 0

for page in pages:

    #Website to scrape
    response = get('https://www.consumerfinance.gov/policy-compliance/enforcement/actions/?page=' + page + '#o-filterable-list-controls')

    #Pause the loop randomly between 5 and 15 seconds
    sleep(randint(5, 15))

    #Track the number of requests made as well as the frequency of the requests
    requests += 1
    elapsed_time = time.time() - start_time
    print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))
    clear_output(wait = True)

    #Warning if there is a non-200 status code
    if response.status_code != 200:
        warnings.warn('Request: {}; Status code: {}'.format(requests, response.status_code))
    
    #Warn and break loop if the number of requests is greater than 25
    if requests > 26:
        warnings.warn('Number of requests was greater than expected.')
        break
    
    #Parse content of request with BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    #Select containers for the 10 cases per page
    case_containers = soup.find_all('article', class_ = 'o-post-preview')

    #For every case 
    for container in case_containers:

            #Name of case
            name = container.a.text
            names.append(name)

            #Date of case
            date = container.find('time', class_ = 'datetime_date').text
            dates.append(date)

            #URL of case
            url = container.a.attrs
            url = url['href']

            urls.append(url)




#Convert scraped data into a pandas dataframe
data = pd.DataFrame({'case': names,
    'date': dates,
    'url': urls})

#Store dataframe as csv file
data.to_csv('consumer_finance.csv')

#Check to see if the data was scraped correctly
print(data.info())
print(data.head())

#Read data into pandas dataframe
df = pd.read_csv('consumer_finance.csv')

#Drop unneeded column
df = df.drop('Unnamed: 0', axis = 1)

#Clean case name
df['case'] = df['case'].str.replace("\n", '')
df['case'] = df['case'].str.strip()

#Complete URL for each case
df['url'] = [('https://www.consumerfinance.gov' + str(url)) for url in df.url]

#Save to csv 
df.to_csv('consumer_finance.csv')